import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd

# D√©finition des URLs des cat√©gories
categories = {
    "R√©frig√©rateurs & Cong√©lateurs": "https://www.expat-dakar.com/refrigerateurs-congelateurs",
    "Climatisation": "https://www.expat-dakar.com/climatisation",
    "Cuisini√®res & Fours": "https://www.expat-dakar.com/cuisinieres-fours",
    "Machines √† Laver": "https://www.expat-dakar.com/machines-a-laver"
}

# Nombre maximal de pages √† scraper par cat√©gorie (1 √† 50 pages)
pages_par_categorie = {
    "R√©frig√©rateurs & Cong√©lateurs": list(range(1,221)),
    "Climatisation": list(range(1,126)),
    "Cuisini√®res & Fours": list(range(1, 101)),
    "Machines √† Laver": list(range(1,71))
}

# Fonction de scraping
def scrape_data(base_url, num_pages):
    data = []
    
    for page in range(1, num_pages + 1):
        url = f"{base_url}?page={page}"  # Construction de l'URL de la page
        response = requests.get(url)
        
        if response.status_code != 200:
            st.error(f"Erreur lors de la r√©cup√©ration de la page {page}")
            continue
        
        soup = BeautifulSoup(response.text, "html.parser")
        conteneurs = soup.find_all("article", class_="listing-card")

        for conteneur in conteneurs:
            try:
                details = conteneur.find('div', {'class': 'listing-card__header__title'}).text.strip()
                etat = conteneur.find('span', {'class': 'listing-card__header__tags__item listing-card__header__tags__item--condition listing-card__header__tags__item--condition_new'}).text
                adresse = conteneur.find('div', {'class': 'listing-card__header__location'}).text.strip().replace(',\n', '').strip()
                prix = conteneur.find('span', {'class': 'listing-card__price__value 1'}).text.strip().replace('\u202f', '').replace(' F Cfa', '')
                image_lien = soup.find('img', {'class': 'listing-card__image__resource vh-img'})['src']
                data.append({
                    "Details": details,
                    "Etat": etat,
                    "Adresse": adresse,
                    "Prix": prix,
                    "Image": image_lien,
                })
            except AttributeError:
                continue
    
    return pd.DataFrame(data)

# Interface Streamlit
st.title("üåê Scraper sur Expat-Dakar")
st.write("S√©lectionnez la cat√©gorie et le nombre de pages √† scraper.")

# S√©lection de la cat√©gorie
categorie = st.selectbox("Choisissez une cat√©gorie :", list(categories.keys()))

# S√©lection des pages disponibles en fonction de la cat√©gorie
pages_disponibles = pages_par_categorie[categorie]

# Liste d√©roulante pour le nombre de pages √† scraper
num_pages = st.selectbox("S√©lectionnez le nombre de pages √† scraper :", pages_disponibles)

# Bouton pour lancer le scraping
if st.button("Lancer le Scraping"):
    with st.spinner("Scraping en cours... ‚è≥"):
        df = scrape_data(categories[categorie], num_pages)
    
    if not df.empty:
        st.success(f"‚úÖ {len(df)} annonces r√©cup√©r√©es avec succ√®s !")
        st.dataframe(df)  # Affichage du tableau interactif
        
        # Bouton pour t√©l√©charger les donn√©es en CSV
        csv = df.to_csv(index=False).encode("utf-8")
        st.download_button(label="üì• T√©l√©charger les donn√©es en CSV", data=csv, file_name="annonces_expat_dakar.csv", mime="text/csv")
    else:
        st.warning("‚ö†Ô∏è Aucune annonce trouv√©e. Essayez avec plus de pages !")